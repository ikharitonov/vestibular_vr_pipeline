{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Read H5 files to csv\n",
    "Read h5 files created in Data_extration.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from harp_resources import process, utils\n",
    "import h5py\n",
    "#from analysis_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse_info = {'B2M1': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B2M4': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B2M5': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B2M6': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B3M1': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B3M2': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B3M3': {'sex': 'F', 'area': 'V1'},\n",
    "              'B3M4': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B3M5': {'sex': 'M', 'area': 'V2M'},\n",
    "              'B3M6': {'sex': 'F', 'area': 'V2M'},\n",
    "              'B3M7': {'sex': 'F', 'area': 'V2M'},\n",
    "              'B3M8': {'sex': 'F', 'area': 'V2M'},\n",
    "              'B0M0': {'sex': 'F', 'area': 'V2M'},\n",
    "             }\n",
    "\n",
    "session_info = {'220824': 'day1',\n",
    "                '230824': 'day2',\n",
    "                '190824': 'day1',\n",
    "                '200824': 'day2',\n",
    "                '120824': 'day1',\n",
    "                '130824': 'day2',\n",
    "                '070824': 'day1',\n",
    "                '080824': 'day2',\n",
    "               }\n",
    "\n",
    "#'SleapVideoData2'] = ['Ellipse.Diameter', 'Ellipse.Center.X', 'Ellipse.Center.Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Defining paths for grab or G8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Volumes/RanczLab/20240730_Mismatch_Experiment/G8_MMclosed-and-Regular_130824'#Enter root path\n",
    "rootdir = '/Users/nora/Desktop/Cohort0_GCaMP_example/2024-08-08T10-05-26_B3M3'\n",
    "h5_paths = []\n",
    "eventpaths = []\n",
    "for dirpath, subdirs, files in os.walk(rootdir):\n",
    "    for x in files:\n",
    "        if '.h5' in x:\n",
    "            eventpaths.append(dirpath)\n",
    "            h5_paths.append(dirpath+'/'+x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "h5_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression unit testing\n",
    "'''h5_paths = ['/Volumes/RanczLab/20240730_Mismatch_Experiment/Fake_data_080824/Test_streams_B0M0/resampled_streams_Test_streams_B0M0.h5',\n",
    "            '/Volumes/RanczLab/20240730_Mismatch_Experiment/Fake_data_070824/Test_streams_B0M0/resampled_streams_Test_streams_B0M0.h5']\n",
    "eventpaths = ['/Volumes/RanczLab/20240730_Mismatch_Experiment/Fake_data_080824/Test_streams_B0M0',\n",
    "             '/Volumes/RanczLab/20240730_Mismatch_Experiment/Fake_data_070824/Test_streams_B0M0']'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Loading data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def load_h5_streams_to_dict(data_paths):\n",
    "    '''\n",
    "    Takes list of H5 file paths and, loads streams into dictionary, and save to dictionary named by mouse ID\n",
    "    '''\n",
    "    reconstructed_dict = {}  # Dictionary to save streams\n",
    "    \n",
    "    for input_file in data_paths:\n",
    "        \n",
    "        name = input_file.split('/')[-1][-7:-3]  # Extract mouse ID from file name\n",
    "        \n",
    "        if not os.path.exists(input_file):\n",
    "            print(f'ERROR: {input_file} does not exist.')\n",
    "            return None\n",
    "        \n",
    "        with h5py.File(input_file, 'r') as h5file:\n",
    "            print(f'reconstructing streams for mouse {name}, from session folder: {input_file.split(\"/\")[-3]}')\n",
    "            \n",
    "            common_index = h5file['HARP_timestamps'][:]\n",
    "            reconstructed_streams = {}\n",
    "            \n",
    "            for source_name in h5file.keys():\n",
    "                if source_name == 'HARP_timestamps':\n",
    "                    continue\n",
    "                \n",
    "                reconstructed_streams[source_name] = {}\n",
    "                source_group = h5file[source_name]\n",
    "                \n",
    "                for stream_name in source_group.keys():\n",
    "                    stream_data = source_group[stream_name][:]\n",
    "                    length_difference = len(common_index) - len(stream_data)\n",
    "                    \n",
    "                    # Pad or truncate to match common_index length\n",
    "                    if len(stream_data) < len(common_index):\n",
    "                        padding = np.full(len(common_index) - len(stream_data), np.nan)\n",
    "                        stream_data = np.concatenate([stream_data, padding])\n",
    "                        print(f\"{source_name} - {stream_name}: Length difference: {length_difference}\")\n",
    "                        print(f\"missing data, advicable to ensure correct alignment \\n \")\n",
    "                    elif len(stream_data) > len(common_index):\n",
    "                        stream_data = stream_data[:len(common_index)]\n",
    "                    \n",
    "                    reconstructed_streams[source_name][stream_name] = pd.Series(data=stream_data, index=common_index)\n",
    "        if name not in reconstructed_dict.keys():   \n",
    "            reconstructed_dict[name] = reconstructed_streams\n",
    "            print(f'  --> {name} streams reconstructed and added to dictionary \\n')\n",
    "        else: \n",
    "            reconstructed_dict[f'{name}_2'] = reconstructed_streams\n",
    "            print(f'  --> {name} streams_2 reconstructed and added to dictionary \\n')\n",
    "    \n",
    "    return reconstructed_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_dict_dict = load_h5_streams_to_dict(h5_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_dict_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_dict_dict['B3M3']#['Photometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make a cut_info dict for the mouse with missing data\n",
    "#cut_info = {'B2M5': 521, 'B3M2':174}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes(stream_dict_dict, cut_info = {}):\n",
    "    data_dict = {}\n",
    "    for mouse, streamdict in stream_dict_dict.items():\n",
    "        \n",
    "        print(f'\\n--Making dataframe for {mouse}--')\n",
    "        #Getting fluorescence traces\n",
    "        try: \n",
    "            fluorescence = streamdict['Photometry']['470_dfF'] #Using '470_dfF' only\n",
    "        except KeyError:\n",
    "            fluorescence = streamdict['Photometry']['CH1-470']\n",
    "        print('flourescence 470 extracted')\n",
    "    \n",
    "        #Getting mouse movement data and converting to cm / second\n",
    "        movementX = process.running_unit_conversion(streamdict['H1']['OpticalTrackingRead0X(46)'])*100\n",
    "        movementY = process.running_unit_conversion(streamdict['H1']['OpticalTrackingRead0Y(46)'])*100\n",
    "        print('movement on x and Y axis extracted')\n",
    "    \n",
    "        #Getting eye movements and pupil diameter \n",
    "        #'SleapVideoData2' = ['Ellipse.Diameter', 'Ellipse.Center.X', 'Ellipse.Center.Y']\n",
    "        if 'SleapVideoData2' in streamdict:\n",
    "            eye_center_x = streamdict['SleapVideoData2']['Ellipse.Center.X']\n",
    "            eye_center_y = streamdict['SleapVideoData2']['Ellipse.Center.Y']\n",
    "            eye_diameter = streamdict['SleapVideoData2']['Ellipse.Diameter']\n",
    "            print('eye movement data extracted')\n",
    "        else: \n",
    "            print('There was no eye movement data available for ', mouse)\n",
    "    \n",
    "        #Getting visual stimuli event times\n",
    "        event = streamdict['ONIX']['Photodiode']\n",
    "        print('photdiode halt info extracted')\n",
    "        \n",
    "        time = movementX.index - movementX.index[0]\n",
    "        print('time in seconds from 0 extracted form X direction movement')\n",
    "        \n",
    "        dict = {'470_dfF': fluorescence, 'movementX': movementX, 'movementY': movementY, 'event': event,\n",
    "            'Seconds': time}\n",
    "        #dict = {'470_dfF': fluorescence, 'movementX': movementX, 'movementY': movementY, 'event': event,\n",
    "         #   'TimeStamp': time, 'eye_x': eye_center_x, 'eye_y': eye_center_y, 'pupil_diameter': eye_diameter}\n",
    "        \n",
    "        df = pd.DataFrame(dict)\n",
    "        print('dataframe created with columns: ', df.columns)\n",
    "        #if mouse in cut_info:\n",
    "            #df = df.iloc[:-cut_info[mouse]]\n",
    "        \n",
    "        df['event'] = df['event'].astype(bool) #In case column is not bool\n",
    "        #Reversing, so that a halt appearst when 'event'==True\n",
    "        df['event'] = ~df['event']\n",
    "        print('Event column as bool, True values corresponding to halts')\n",
    "        \n",
    "        df.reset_index(inplace=False)\n",
    "        \n",
    "        data_dict[mouse]= df\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataframes(stream_dict_dict, fluorescense_traces = ['470_dfF'], cut_info={}):\n",
    "    data_dict = {}\n",
    "    for mouse, streamdict in stream_dict_dict.items():\n",
    "        \n",
    "        print(f'\\n--Making dataframe for {mouse}--')\n",
    "        # Getting fluorescence traces\n",
    "        fluorescence_dict = {}\n",
    "        for trace in fluorescense_traces:\n",
    "            try: \n",
    "                fluorescence_dict[trace] = streamdict['Photometry'][trace] \n",
    "            except KeyError:\n",
    "                print(f'Trace {trace} not available')\n",
    "                pass\n",
    "    \n",
    "        # Getting mouse movement data and converting to cm / second\n",
    "        movementX = process.running_unit_conversion(streamdict['H1']['OpticalTrackingRead0X(46)']) * 100\n",
    "        movementY = process.running_unit_conversion(streamdict['H1']['OpticalTrackingRead0Y(46)']) * 100\n",
    "        print('movement on x and Y axis extracted')\n",
    "    \n",
    "        # Getting eye movements and pupil diameter\n",
    "        # 'SleapVideoData2' = ['Ellipse.Diameter', 'Ellipse.Center.X', 'Ellipse.Center.Y']\n",
    "        eye_data_available = 'SleapVideoData2' in streamdict\n",
    "        if eye_data_available:\n",
    "            eye_center_x = streamdict['SleapVideoData2']['Ellipse.Center.X']\n",
    "            eye_center_y = streamdict['SleapVideoData2']['Ellipse.Center.Y']\n",
    "            eye_diameter = streamdict['SleapVideoData2']['Ellipse.Diameter']\n",
    "            print('eye movement data extracted')\n",
    "        else: \n",
    "            print('There was no eye movement data available for ', mouse)\n",
    "    \n",
    "        # Getting visual stimuli event times\n",
    "        event = streamdict['ONIX']['Photodiode']\n",
    "        print('photodiode halt info extracted')\n",
    "        \n",
    "        time = movementX.index - movementX.index[0]\n",
    "        print('time in seconds from 0 extracted from X direction movement')\n",
    "        \n",
    "        # Creating the dictionary for the DataFrame\n",
    "        data_dict_for_df = {\n",
    "            'movementX': movementX, \n",
    "            'movementY': movementY, \n",
    "            'event': event,\n",
    "            'Seconds': time\n",
    "        }\n",
    "        data_dict_for_df.update(fluorescence_dict)\n",
    "        \n",
    "        # Adding eye data to the dictionary if available\n",
    "        if eye_data_available:\n",
    "            data_dict_for_df['eye_x'] = eye_center_x\n",
    "            data_dict_for_df['eye_y'] = eye_center_y\n",
    "            data_dict_for_df['pupil_diameter'] = eye_diameter\n",
    "        \n",
    "        df = pd.DataFrame(data_dict_for_df)\n",
    "        print('dataframe created with columns: ', df.columns)\n",
    "        \n",
    "        # if mouse in cut_info:\n",
    "        #     df = df.iloc[:-cut_info[mouse]]\n",
    "        \n",
    "        df['event'] = df['event'].astype(bool)  # In case column is not bool\n",
    "        # Reversing, so that a halt appears when 'event'==True\n",
    "        df['event'] = ~df['event']\n",
    "        print('Event column as bool, True values corresponding to halts')\n",
    "        \n",
    "        df.reset_index(inplace=False)\n",
    "        \n",
    "        data_dict[mouse] = df\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = make_dataframes(stream_dict_dict, fluorescense_traces = ['470_dfF', 'z_470'])\n",
    "names = [name for name in data_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[names[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mouse, df in data_dict.items():\n",
    "    percent_true = (len(df.loc[df['event']==True])*100)/len(df)\n",
    "    print(f'for {mouse} the True values makes up {percent_true:.2f} % of the total df lenght' )\n",
    "    if percent_true > 50:\n",
    "        print('This is more than 50 %, which may be too much, consider inversing True/False or check experiment protocol for mouse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Loading Experiment events and session info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "eventpath = '/Users/nora/Desktop/Cohort0_GCaMP_example/2024-08-08T10-05-26_B3M3/'\n",
    "eventpath.split('/')[-2].split('_')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict = {}\n",
    "for eventpath in eventpaths:\n",
    "    eventpath = '/Users/nora/Desktop/Cohort0_GCaMP_example/2024-08-08T10-05-26_B3M3/'\n",
    "    ExpEvents = process.read_ExperimentEvents(Path(eventpath))\n",
    "    ExpEvents.set_index('Seconds', inplace = True)\n",
    "    ExpEvents.index = ExpEvents.index.round(4)\n",
    "    name = eventpath.split('/')[-1][-4:]\n",
    "    ExpEvents['experiment'] = eventpath.split('/')[-2].split('_')[-1]\n",
    "    for key, item in session_info.items():\n",
    "        if key in eventpath.split('/')[-2]:\n",
    "            ExpEvents['session']=item\n",
    "    if name not in event_dict.keys():  \n",
    "        event_dict[name] = ExpEvents\n",
    "    else:\n",
    "        event_dict[f'{name}_2'] = ExpEvents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict['B3M3'] = event_dict['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dict['B3M3']['experiment']='closedopenMM'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Adding events (and non-events) and session info to data\n",
    "1) add_experiment_events() takes the event data, and inserts the values for every timepoint into an event column in the main data\n",
    "* Sometimes, there are multiple event strings during the same timepoint. Then, if a crucial event takes place during this, this one will be priotized\n",
    "* The crucial events can be defined from strings they contain in the first line of the add_experiment_events function.\n",
    "* In no crucial event are at that timepoint, all the events will be assinged to the timepoint in the main df as one string, seperated by a comma (use .split(',') later if it becomes necessary to seperate them during analysis)\n",
    "\n",
    "2) The No_halt events are used to make a column where the no-halt events are used to make a bool similar to the 'event' (halt) column\n",
    "   * These can later be used as control as they appear when there could have been a halt but there was none\n",
    "   * The number of no-halt events is controlled to ensure that all of them were actually used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_experiment_events(data_dict, events_dict, mouse_info):\n",
    "    # Iterate over each mouse key in the dictionaries\n",
    "    for mouse_key in data_dict:\n",
    "        # Retrieve the main and event DataFrames\n",
    "        main_df = data_dict[mouse_key]\n",
    "        event_df = events_dict[mouse_key]\n",
    "\n",
    "        # Ensure both indices are sorted\n",
    "        main_df = main_df.sort_index()\n",
    "        event_df = event_df.sort_index()\n",
    "\n",
    "        # Perform a merge_asof on the index to add 'Value' as 'ExperimentEvents' with backward matching\n",
    "        merged_df = pd.merge_asof(\n",
    "            main_df,\n",
    "            event_df[['Value']],  # Only select the 'Value' column from event_df\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            direction='backward',\n",
    "            tolerance=0  # Adjust tolerance for matching on the index\n",
    "        )\n",
    "\n",
    "        # Rename the 'Value' column to 'ExperimentEvents'\n",
    "        if 'ExperimentEvents' in merged_df.columns:\n",
    "            merged_df['ExperimentEvents'] = merged_df.pop('Value')  # Replace existing column with the new 'Value' column\n",
    "            print(f'Pre-existing ExperimentEvents column was replaced with new for {mouse_key}')\n",
    "        else:\n",
    "            merged_df = merged_df.rename(columns={'Value': 'ExperimentEvents'})  # Add new column\n",
    "            print(f'Added new ExperimentEvents for {mouse_key}')\n",
    "\n",
    "        # Add metadata from event_df\n",
    "        #merged_df['Experiment'] = event_df['experiment'].unique()[0]\n",
    "        #merged_df['Session'] = event_df['session'].unique()[0]\n",
    "\n",
    "        # Add mouse ID, sex, and brain area\n",
    "        mouse_info_name = mouse_key[:4]\n",
    "        merged_df['mouseID'] = mouse_info_name\n",
    "        merged_df['sex'] = mouse_info[mouse_info_name]['sex']\n",
    "        merged_df['area'] = mouse_info[mouse_info_name]['area']\n",
    "\n",
    "        # Update the dictionary with the merged DataFrame\n",
    "        data_dict[mouse_key] = merged_df\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = add_experiment_events(data_dict, event_dict,mouse_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict[names[0]].ExperimentEvents.unique() #Check random mouse to see what events are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = process.add_no_halt_column(data_dict, event_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "#### Add block columns\n",
    "For each mouse and corresponding df, update the df to include columns for each block of the experiment for easy slicing later in analysis.\n",
    "The add_block_columns() function will also test if each of the created block columns contains at least one True value and that there are no temporal overlaps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in data_dict.items():\n",
    "    print('\\n updating data for ', name,'...')\n",
    "    blocks_added_df = process.add_block_columns(df, event_dict[name])\n",
    "    blocks_added_df.replace({})\n",
    "    data_dict[name] = blocks_added_df\n",
    "\n",
    "process.check_block_overlap(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "process.check_block_overlap(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### The downsample_data function can be used to make the datset smaller. \n",
    "1) Make an empty dict to fill with downsampled versions of the dfs.\n",
    "2) Loop though the mice and dfs, and use the function for each\n",
    "3) Set the name of the time column to use and decide on the frequency of the output df datapoints\n",
    "        * Ensure that all the columns that you want to keep has a corresponding dict key in aggregation_functions in the downsample_data() funciton. \n",
    "4) Assign the resulting df to the corresponding mouse\n",
    "5) test_event_numbers as a way to test if all events (no-halt events as they are frequent, can be changed) survived the downsampling. \n",
    "\n",
    "NB: Can be slow with large datasets, check your email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_data(df, time_col='Seconds', interval=0.001):\n",
    "    '''\n",
    "    Uses pandas resample and aggregate functions to downsample the data to the desired interval. \n",
    "    * Note: Aggregation functions must be applied for each variable that is to be included.\n",
    "    https://pandas.pydata.org/docs/reference/api/pandas.core.resample.Resampler.aggregate.html\n",
    "    * Note: because the donwsampling keeps the first non-NaN value in each interval, some values could be lost.\n",
    "    '''\n",
    "    # Convert the Seconds column to a TimedeltaIndex\n",
    "    df = df.set_index(pd.to_timedelta(df[time_col], unit='s'))\n",
    "\n",
    "    #define aggregation functions for all possible columns\n",
    "    aggregation_functions = {\n",
    "        '470_dfF': 'mean', # takes the mean signal of the datapoints going into each new downsampled datapoint\n",
    "        '560_dfF': 'mean',\n",
    "        'movementX': 'mean',\n",
    "        'movementY': 'mean',\n",
    "        'event': 'any', # events column is a bool, and if there is any True values in the interval, the downsampled datapoint will be True\n",
    "        'ExperimentEvents': lambda x: x.dropna().iloc[0] if not x.dropna().empty else None, #first non-NaN value in the interval \n",
    "        'Experiment': 'first', # All values should be the same, so it can always just take the first string value\n",
    "        'Session': 'first',\n",
    "        'mouseID': 'first',\n",
    "        'sex': 'first',\n",
    "        'area': 'first',\n",
    "        'No_halt': 'any', \n",
    "        'LinearMismatch_block': 'any', \n",
    "        'LinearPlaybackMismatch_block': 'any',\n",
    "        'LinearRegular_block': 'any',\n",
    "        'LinearClosedloopMismatch_block':'any',\n",
    "        'LinearRegularMismatch_block':'any',\n",
    "        'LinearNormal_block':'any',\n",
    "    }\n",
    "\n",
    "    # Filter aggregation_functions to only include columns present in df\n",
    "    aggregation_functions = {key: func for key, func in aggregation_functions.items() if key in df.columns}\n",
    "\n",
    "    print('downsampling...')\n",
    "    # Resample with the specified interval and apply the filtered aggregations\n",
    "    downsampled_df = df.resample(f'{interval}s').agg(aggregation_functions)\n",
    "\n",
    "    # Reset the index to make the Seconds column normal again\n",
    "    downsampled_df = downsampled_df.reset_index()\n",
    "    downsampled_df[time_col] = downsampled_df[time_col].dt.total_seconds()  # Convert Timedelta back to seconds\n",
    "\n",
    "    # Forward fill for categorical columns if needed, only if they exist in downsampled_df\n",
    "    categorical_cols = ['Experiment', 'Session', 'mouseID', 'sex', 'area']\n",
    "    for col in categorical_cols:\n",
    "        if col in downsampled_df.columns:\n",
    "            downsampled_df[col] = downsampled_df[col].ffill()\n",
    "\n",
    "    # Remove consecutive duplicate values in the 'ExperimentEvents' column, if it exists\n",
    "    if 'ExperimentEvents' in downsampled_df.columns:\n",
    "        downsampled_df['ExperimentEvents'] = downsampled_df['ExperimentEvents'].where(\n",
    "            downsampled_df['ExperimentEvents'] != downsampled_df['ExperimentEvents'].shift()\n",
    "        )\n",
    "\n",
    "    return downsampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_event_numbers(downsampled_data, original_data, mouse):\n",
    "    '''\n",
    "    Counts number of True values in the No_halt columns in the original and the downsampled data\n",
    "    This will indicate whether information was lost in the downsampling.\n",
    "    If the original events somehow has been upsampled previously (for example if the tolerance was set too high in add_experiment_events()), \n",
    "    repeatings of the same event can also lead to fewer True events in the downsampled df.\n",
    "    '''\n",
    "    nohalt_down = len(downsampled_data.loc[downsampled_data['No_halt']==True])\n",
    "    nohalt_original = len(original_data.loc[original_data['No_halt']==True])\n",
    "    if nohalt_down != nohalt_original:\n",
    "        print(f'mouse{mouse}')\n",
    "        print(f'There are actually {nohalt_original} no-halts, but the downsampled data only contains {nohalt_down}')\n",
    "        print('Should re-run the downsampling. Try changing interval lenght. Othewise, consider not downsampling\\n')\n",
    "    if nohalt_down == nohalt_original:\n",
    "        print(f'mouse{mouse}')\n",
    "        print(f'There are {nohalt_original} no-halts, and downsampled data contains {nohalt_down}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dict = {}\n",
    "for mouse, df in data_dict.items():\n",
    "    downsampled_df = process.downsample_data(df, time_col='Seconds', interval=0.001)\n",
    "    downsampled_dict[mouse] = downsampled_df\n",
    "    process.test_event_numbers(downsampled_df, df, mouse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downsampled_dict[names[0]].loc[downsampled_dict[names[0]].No_halt == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "### Concat and reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data = pd.concat([Data for Data in downsampled_dict.values()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "# Set a file name and save\n",
    "\n",
    "!!! Make sure to change file save names before running the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'day1' in All_data.Session.values:\n",
    "    All_data.to_csv('Mismatch_analysis/G8_MMclosed_regular_session1.csv', index=False) #Change name\n",
    "if 'day2' in All_data.Session.values:\n",
    "    All_data.to_csv('Mismatch_analysis/G8_MMclosed_regular_session2.csv', index=False) #Change name\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_dict['B3M3'].to_csv('Mismatch_analysis/B3M3_G8_MMclosed_session1.csv', index=False) #Change name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
