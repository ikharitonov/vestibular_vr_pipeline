{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from preprocess_functions import preprocess #preprocess is a class containing all the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Set up paths and params, then load data and fill object\n",
    "path is the root_data path as defined in https://github.com/ikharitonov/vestibular_vr_pipeline/issues/25)\n",
    "\n",
    "Select sensors if sensor-specific (and not \"auto\") filtering is used. 'G8m', 'g5-HT3', 'rG1' or available sensors in the function, otherwise asks for user input for half decay time in ms.\n",
    "\n",
    "Target area is the intended area, not verified by histology yet. Added to self.info dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/rancze/Documents/Data/vestVR/Cohort2_test/2025-02-13T12-41-57'\n",
    "sensors = {'470':'g5-HT3', '560':'rG1', '410':'isosbestic'}\n",
    "plot_info = ('(addition info here)') #e.g. retro inj in SC\n",
    "target_area = ('X')\n",
    "detrendmethod = 'subtractive' #subtractive or divisive\n",
    "motion_correction = True #boolean, only applies to 410/470 channels currently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an object which will contain an increasing amount of information as functions are called on\n",
    "processed = preprocess(path, sensors)\n",
    "# extract all relevant and irrelevant info from the Fluorescence.csv file which contains the metadata \n",
    "processed.info = processed.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads Events.csv and Fluorescence-unaligned.csv\n",
    "#Aligns to 470 nm timestamps (assumes 470 exists) and cuts data if needed (almost never)\n",
    "#Returns processed dataframes below\n",
    "(\n",
    "    processed.rawdata, \n",
    "    processed.data, \n",
    "    processed.data_seconds, \n",
    "    processed.signals, \n",
    ") = processed.create_basic(\n",
    "    cutstart = False,\n",
    "    cutend = False,\n",
    "    target_area = target_area, \n",
    "    motion = motion_correction\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "All the sigals are low pass filtered using a butterworth filter.  \n",
    "method = \"auto\" cutoff frequncy ~sample_rate/2 Hz  \n",
    "method = \"sensor\" cutoff frequency is determined in the function using the sensors dictionary  \n",
    "savefig = False by default, True will save the figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.filtered = processed.low_pass_filt(method = \"auto\", plot=False, x_start=0, x_end=100) #for inspection\n",
    "processed.filtered = processed.low_pass_filt(method = \"auto\", plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "A double exponential fit is made to account for sources of bleaching and the signal is corrected.  \n",
    "method = \"subtractive\" assumes bleaching is sensor-independent (e.g. autofluorescence)  \n",
    "method = \"divisive\" assumes bleaching comes from the sensor. This is most plausible.  \n",
    "savefig = False by default, True will save the figure  \n",
    "**N.B.** divisive detrended data is already dF/F. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.detrended, processed.exp_fits = processed.detrend(plot = False, method = detrendmethod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Motion correction\n",
    "There is a motion correction function that can be used. It is now set to use the 560 nm signal, because of my doubts with the relevans of the 410 nm signal as isosbestic trace. For now, I recommend not running this one.\n",
    "Check function before use, not checked in Jan 2025. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.motion_corrected = processed.movement_correct(plot = True, iso_ch = 410, signal_ch = 470)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Delta F / F\n",
    "\n",
    "WITH divisive detrending, this is not needed\n",
    "\n",
    "This is a standard way of calculating the detla F over F signal, i.e. the % change in signal. I do think it is a bit weird to use the detrending exponential fit again. I have wondered if I should change it to just a linear fit to the current detrended signal. For now I do this based on the fiber photometry primer paper code: https://github.com/ThomasAkam/photometry_preprocessing/blob/master/Photometry%20data%20preprocessing.ipynb\n",
    "\n",
    "Again, 'motion' can be set to True, bu tis defaulth False\n",
    "savefig = False by default, True will save the figure "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### THIS NOW BREAKS FOR SUBTRACTIVE AND MOTION CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.deltaF_F = processed.get_deltaF_F(plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Z-scoring\n",
    "Standard Z-scoring of the dF/F \n",
    "motion = False does not use motion corrected signal  \n",
    "savefig = False by default, True will save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.zscored = processed.z_score(plot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.cross_correlate_signals(col1='470', col2='560', plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.show_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.plot_all_signals(sensors, plot_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Save it as a .csv files\n",
    "from Hilde  \n",
    "This function will lead to it all being saved as a csv file which can easily be read as a pandas dataframe when the data is to be analysed.\n",
    "First it is the info csv, which I for now save, but never actually use...\n",
    "Then it is the main csv file which is very useful indeed. For this one you can add Events = True to also save the events, and motion_correct = True if you have doen motion correction and want to use this.The only difference for the latter, is really that it also saved the motion corrected raw signal. Regardless, if you did use motion correction for deltaF and z-score, this is the version that will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again it ensures that the folder to save in already exists, since the csv must have somewhere to be\n",
    "processed.info_csv = processed.write_info_csv()\n",
    "processed.data_csv = processed.write_preprocessed_csv() #optional: Events = True; motion = False not impleneted yet\n",
    "#optional:, motion_correct = True, Onix_align =False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Have a look\n",
    "By importing pandas, you can now read the file, by compying the path from above and adding 'preprocessed.csv' which is the name of your new file. Sorry about the unnamed file. It can be removed. I'll do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(processed.save_path+'/Events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(processed.save_path+'/Processed_fluorescence.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
