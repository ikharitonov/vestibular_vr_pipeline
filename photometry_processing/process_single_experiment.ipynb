{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from preprocess_functions import preprocess #preprocess is a class containing all the function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Set up paths and params, then load data and fill object\n",
    "path is the root_data path as defined in https://github.com/ikharitonov/vestibular_vr_pipeline/issues/25)\n",
    "\n",
    "Select sensors if sensor-specific (and not \"auto\") filtering is used. 'G8m', 'g5-HT3', 'rG1' or available sensors in the function, otherwise asks for user input for half decay time in ms.\n",
    "\n",
    "Target area is the intended area, not verified by histology yet. Added to self.info dictionary.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/rancze/Documents/Data/vestVR/Cohort1/VestibularMismatch_day1/B6J2717-2024-12-12T12-54-49'\n",
    "sensors = {'470':'g5-HT3', '560':'rG1', '410':'g5-HT3'}\n",
    "target_area = ('X') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an object which will contain an increasing amount of information as functions are called on\n",
    "processed = preprocess(path, sensors)\n",
    "# extract all relevant and irrelevant info from the Fluorescence.csv file which contains the metadata \n",
    "processed.info = processed.get_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads Events.csv and Fluorescence-unaligned.csv\n",
    "#Aligns to 470 nm timestamps (assumes 470 exists) and cuts data if needed (almost never)\n",
    "#Returns processed dataframes below\n",
    "(\n",
    "    processed.rawdata, \n",
    "    processed.data, \n",
    "    processed.data_seconds, \n",
    "    processed.signals, \n",
    ") = processed.create_basic(\n",
    "    cutstart = False,\n",
    "    cutend = False,\n",
    "    target_area = target_area\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not sure why is even needed, not using currently, asked Hilde for clarification in issue #4\n",
    "#processed.events = processed.extract_events()\n",
    "'''\n",
    "now we have an element 'events' containing timestamped events\n",
    "for each event there will be a _starts and a _stops and a _event\n",
    " _starts: numpy nans for all rows except at the time stamp where the event starts\n",
    " _stops: numpy nans for all rows except at the time stamp where the event stops\n",
    " _events: False whenever the event did not take place, and True while it did take place\n",
    " The event is named the same as was as it was recorded\n",
    "'''\n",
    "#processed.events\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "All the sigals are low pass filtered using a butterworth filter.  \n",
    "method = \"auto\" cutoff frequncy ~sample_rate/2 Hz  \n",
    "method = \"sensor\" cutoff frequency is determined in the function using the sensors dictionary  \n",
    "savefig = False by default, True will save the figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.filtered = processed.low_pass_filt(method = \"auto\", plot=True, x_start=0, x_end=100) #for inspection\n",
    "processed.filtered = processed.low_pass_filt(method = \"auto\", plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Detrending\n",
    "A double exponential fit is made to account for sources of bleaching and the signal is corrected.  \n",
    "method = \"subtractive\" assumes bleaching is sensor-independent (e.g. autofluorescence)  \n",
    "method = \"divisive\" assumes bleaching comes from the sensor. This is most plausible.  \n",
    "savefig = False by default, True will save the figure  \n",
    "**N.B.** divisive detrended data is already dF/F. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.data_detrended, processed.exp_fits = processed.detrend(plot = True, method = \"divisive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Motion correction\n",
    "There is a motion correction function that can be used. It is now set to use the 560 nm signal, because of my doubts with the relevans of the 410 nm signal as isosbestic trace. For now, I recommend not running this one.\n",
    "Check function before use, not checked in Jan 2025. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed.motion_corr = processed.movement_correct(plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "### Z-scoring\n",
    "Standard Z-scoring of the signal.  \n",
    "If divisive detrending was used, this is the z-scored dF/F.  \n",
    "If subtractive detrending was used, this is the z-scored signal.  \n",
    "motion = False does not use motion corrected signal  \n",
    "savefig = False by default, True will save the figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.zscored = processed.z_score(motion = False, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Delta F / F\n",
    "\n",
    "WITH divisive detrending, this is not needed\n",
    "\n",
    "This is a standard way of calculating the detla F over F signal, i.e. the % change in signal. I do think it is a bit weird to use the detrending exponential fit again. I have wondered if I should change it to just a linear fit to the current detrended signal. For now I do this based on the fiber photometry primer paper code: https://github.com/ThomasAkam/photometry_preprocessing/blob/master/Photometry%20data%20preprocessing.ipynb\n",
    "\n",
    "Again, 'motion' can be set to True, bu tis defaulth False\n",
    "savefig = False by default, True will save the figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.deltaF_F = processed.get_deltaF_F(motion = False, plot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Add crucial info\n",
    "`processed.crucial_info = processed.add_crucial_info()`  \n",
    "This has been removed and now added to the self.Info instead of columns in the saved fluorescence csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show structure of data frame to be saved \n",
    "processed.show_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.plot_all_signals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Save it as a .csv files\n",
    "from Hilde  \n",
    "This function will lead to it all being saved as a csv file which can easily be read as a pandas dataframe when the data is to be analysed.\n",
    "First it is the info csv, which I for now save, but never actually use...\n",
    "Then it is the main csv file which is very useful indeed. For this one you can add Events = True to also save the events, and motion_correct = True if you have doen motion correction and want to use this.The only difference for the latter, is really that it also saved the motion corrected raw signal. Regardless, if you did use motion correction for deltaF and z-score, this is the version that will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#again it ensures that the folder to save in already exists, since the csv must have somewhere to be\n",
    "processed.info_csv = processed.write_info_csv()\n",
    "processed.data_csv = processed.write_preprocessed_csv()\n",
    "#optional:, motion_correct = True, Onix_align =False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Have a look\n",
    "By importing pandas, you can now read the file, by compying the path from above and adding 'preprocessed.csv' which is the name of your new file. Sorry about the unnamed file. It can be removed. I'll do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(processed.save_path+'/Events.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(processed.save_path+'/Processed_fluorescence.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
