{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Extract and align data from Onix, Harp, Sleap, and photometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import harp\n",
    "\n",
    "from harp_resources import process, utils\n",
    "# from sleap import load_and_process as lp #f-string issue not addressed no"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "### Define paths\n",
    "Define a root directory (e.g. an experimental session) through wich you can loop to get both photometry data and onix data. \n",
    "Check if path names make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = '/Users/rancze/Documents/Data/vestVR/Cohort1/VestibularMismatch_day1'\n",
    "\n",
    "#initialize sets to ensure uniqueness\n",
    "data_paths_set = set()\n",
    "photometry_paths_set = set()\n",
    "\n",
    "data_paths = []\n",
    "photometry_paths = []\n",
    "\n",
    "for dirpath, subdirs, files in os.walk(rootdir):\n",
    "    #data paths\n",
    "    if 'ExperimentEvents' in dirpath:\n",
    "        trimmed_path = os.path.dirname(dirpath)\n",
    "        if trimmed_path not in data_paths_set:\n",
    "            data_paths_set.add(trimmed_path)\n",
    "            data_paths.append(trimmed_path)\n",
    "        photometry_path = os.path.join(trimmed_path, \"photometry\")  # Add \"photometry\" subfolder\n",
    "        if photometry_path not in photometry_paths_set:\n",
    "            photometry_paths_set.add(photometry_path)\n",
    "            photometry_paths.append(photometry_path)\n",
    "\n",
    "# print paths sets in notebook for sanity check\n",
    "print(\"Data Paths:\\n\" + \"\\n\".join(data_paths))\n",
    "print(\"\\nPhotometry Paths:\\n\" + \"\\n\".join(photometry_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Call funcitons to extract data and align timestamps\n",
    "All the function calls needed can be arranged in a function or called one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_extraction_makeh5(data_path, photometry_path, make_h5=False, eyes=False):\n",
    "    print(f'\\n Running extraction for {data_path.split(\"/\")[-1]} from session {data_path.split(\"/\")[-2]}')\n",
    "\n",
    "    # Load session settings and conversions\n",
    "    SessionSettings = utils.read_SessionSettings(Path(data_path), print_contents=True)\n",
    "    conversions = process.calculate_conversions_second_approach(Path(data_path), Path(photometry_path), verbose=False)\n",
    "    streams = utils.load_registers(Path(data_path))\n",
    "\n",
    "    # Load photometry data and align timestamps\n",
    "    Photometry = utils.read_fluorescence(Path(photometry_path))\n",
    "    Photometry['HARP Timestamps'] = conversions['photometry_to_harp_time'](Photometry.index)\n",
    "\n",
    "    # Load OnixAnalog data\n",
    "    OnixAnalogClock = utils.read_OnixAnalogClock(Path(data_path))\n",
    "    OnixAnalogData = utils.read_OnixAnalogData(Path(data_path), binarise=True)\n",
    "    ExperimentEvents = utils.read_ExperimentEvents(Path(data_path))\n",
    "    photodiode_series = pd.Series(OnixAnalogData[:, 0], index=conversions['onix_to_harp_timestamp'](OnixAnalogClock))\n",
    "\n",
    "    print('Adding Photometry, Eye Movements, and Photodiode to the streams')\n",
    "    # Add Photometry and Photodiode streams\n",
    "    streams = process.reformat_and_add_many_streams(\n",
    "        streams, Photometry, 'Photometry', ['470_dfF', 'z_470'], index_column_name='HARP Timestamps'\n",
    "    )\n",
    "    streams = process.add_stream(streams, 'ONIX', photodiode_series, 'Photodiode')\n",
    "\n",
    "    # Process videography data if eyes=True\n",
    "    if eyes:\n",
    "        print('  Checking for and processing videography data...')\n",
    "        try:\n",
    "            # Load videography data\n",
    "            VideoData1, VideoData2, VideoData1_Has_Sleap, VideoData2_Has_Sleap = lp.load_videography_data(data_path)\n",
    "\n",
    "            if VideoData2_Has_Sleap:\n",
    "                print('  Processing VideoData2 with SLEAP data...')\n",
    "\n",
    "                # Interpolate missing data\n",
    "                VideoData2 = VideoData2.interpolate()\n",
    "\n",
    "                # Extract coordinates and compute transformations\n",
    "                columns_of_interest = [\n",
    "                    'left.x', 'left.y', 'center.x', 'center.y', 'right.x', 'right.y',\n",
    "                    'p1.x', 'p1.y', 'p2.x', 'p2.y', 'p3.x', 'p3.y', 'p4.x', 'p4.y',\n",
    "                    'p5.x', 'p5.y', 'p6.x', 'p6.y', 'p7.x', 'p7.y', 'p8.x', 'p8.y'\n",
    "                ]\n",
    "                coordinates_dict = lp.get_coordinates_dict(VideoData2, columns_of_interest)\n",
    "\n",
    "                # Calculate transformations\n",
    "                theta = lp.find_horizontal_axis_angle(VideoData2, 'left', 'center')\n",
    "                center_point = lp.get_left_right_center_point(coordinates_dict)\n",
    "\n",
    "                reformatted_coordinates_dict = lp.get_reformatted_coordinates_dict(coordinates_dict, ['left', 'right', 'center'] + [f'p{i}' for i in range(1, 9)])\n",
    "                centered_coordinates_dict = lp.get_centered_coordinates_dict(reformatted_coordinates_dict, center_point)\n",
    "                rotated_coordinates_dict = lp.get_rotated_coordinates_dict(centered_coordinates_dict, theta)\n",
    "\n",
    "                # Extract ellipse parameters\n",
    "                columns_of_interest = [f'p{i}' for i in range(1, 9)]\n",
    "                ellipse_parameters_data, ellipse_center_points_data = lp.get_fitted_ellipse_parameters(\n",
    "                    rotated_coordinates_dict, columns_of_interest\n",
    "                )\n",
    "\n",
    "                # Compute additional metrics\n",
    "                average_diameter = np.mean([ellipse_parameters_data[:, 0], ellipse_parameters_data[:, 1]], axis=0)\n",
    "\n",
    "                # Prepare SLEAP video data for streams\n",
    "                SleapVideoData2 = process.convert_arrays_to_dataframe(\n",
    "                    ['Seconds', 'Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y'],\n",
    "                    [VideoData2['Seconds'].values, average_diameter, ellipse_parameters_data[:, 2],\n",
    "                     ellipse_center_points_data[:, 0], ellipse_center_points_data[:, 1]]\n",
    "                )\n",
    "\n",
    "                streams = process.reformat_and_add_many_streams(\n",
    "                    streams, SleapVideoData2, 'SleapVideoData2',\n",
    "                    ['Ellipse.Diameter', 'Ellipse.Angle', 'Ellipse.Center.X', 'Ellipse.Center.Y']\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing videography data: {e}\")\n",
    "\n",
    "    # Display timepoint info for streams\n",
    "    _ = process.get_timepoint_info(streams, print_all=True)\n",
    "\n",
    "    # Resample streams\n",
    "    resampled_streams = process.pad_and_resample(streams, resampling_period='1 ms', method='linear')\n",
    "    _ = process.get_timepoint_info(resampled_streams, print_all=True)\n",
    "\n",
    "    # Apply unit conversions for optical tracking sensor streams\n",
    "    print('  Applying linear and angular conversion to Optical tracking sensor streams (cm/sec and degrees/sec)')\n",
    "    resampled_streams['H1']['OpticalTrackingRead0X(46)'] = process.running_unit_conversion(\n",
    "        resampled_streams['H1']['OpticalTrackingRead0X(46)'] * 100\n",
    "    )\n",
    "    resampled_streams['H1']['OpticalTrackingRead0Y(46)'] = process.rotation_unit_conversion(\n",
    "        resampled_streams['H1']['OpticalTrackingRead0Y(46)']\n",
    "    )\n",
    "\n",
    "    print(' - Streams are extracted and can be used or made to h5')\n",
    "\n",
    "    if make_h5:\n",
    "        # Define streams to save, including SLEAP data if processed\n",
    "        streams_to_save_pattern = {\n",
    "            'Photometry': ['470_dfF', 'z_470'],\n",
    "            'ONIX': ['Photodiode'],\n",
    "        }\n",
    "        if eyes:\n",
    "            streams_to_save_pattern['SleapVideoData2'] = ['Ellipse.Diameter', 'Ellipse.Center.X', 'Ellipse.Center.Y']\n",
    "\n",
    "        process.save_streams_as_h5(Path(data_path), resampled_streams, streams_to_save_pattern, SessionSettings)\n",
    "        print('Streams saved as h5 file \\n')\n",
    "\n",
    "    return data_path, resampled_streams, streams_to_save_pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionSettings = utils.read_SessionSettings(Path(data_paths[0]), print_contents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SessionSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_settings_reader = utils.SessionData(\"SessionSettings\")\n",
    "session_settings = utils.load_vers2_3(session_settings_reader, data_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "json.dumps(session_settings, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Call the function\n",
    "Below, the funciton is called with only one path combo, and then with several through a loop\n",
    "If all seems to work, you can set make_h5 to True, or you can use the resampled streams and save pattern daved to the dict made in the loop to make the h5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path, resampled_streams, streams_to_save_pattern = run_extraction_makeh5(\n",
    "    data_paths[1], photometry_paths[1], make_h5=False, eyes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_dict = {}\n",
    "for i, (datapath, photometry_path) in enumerate(zip(data_paths, photometry_paths)):\n",
    "    print(datapath)\n",
    "    print(photometry_path)\n",
    "    stream_dict[f'dataset_{i}'] = {}\n",
    "    data_path, resampled_streams, streams_to_save_pattern = run_extraction_makeh5(\n",
    "    data_paths[1], photometry_paths[1], make_h5=False, eyes=True)\n",
    "    stream_dict[f'dataset_{i}']['resampled_streams'] = resampled_streams\n",
    "    stream_dict[f'dataset_{i}']['streams_to_save_pattern'] = streams_to_save_pattern\n",
    "    stream_dict[f'dataset_{i}']['data_path'] = data_path\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!!! ONLY RUN IF YOU WANT TO MAKE NEW H5 FILES !!!!\n",
    "\n",
    "for dataset, data_dict in stream_dict.items():\n",
    "\n",
    "    process.save_streams_as_h5(Path(data_dict['data_path']), data_dict['resampled_streams'], data_dict['streams_to_save_pattern'], SessionSettings)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Extracting Noras data\n",
    "\n",
    "This is adapted from the instructions given by Andrew for reading the temporary onix digital file versions (the weird ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "photometry_path = '/Volumes/RanczLab/Nora_Cohort1_training/Training_day4/B6J2717-2024-11-28T09-37-55/photometry'\n",
    "data_path = '/Volumes/RanczLab/Nora_Cohort1_training/Training_day4/B6J2717-2024-11-28T09-37-55/'\n",
    "\n",
    "photometry_path = '/Volumes/RanczLab/Nora_Cohort1_training/Visual_mismatch_day1/B6J2717/photometry_processed'\n",
    "data_path = '/Volumes/RanczLab/Nora_Cohort1_training/Visual_mismatch_day1/B6J2717/'\n",
    "\n",
    "h1_datafolder = data_path+'HarpDataH1'\n",
    "h2_datafolder = data_path+'HarpDataH2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(str(photometry_path)+'/Processed_fluorescence.csv') #Processed_fluorescence.csv #Fluorescence.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from andrew:\n",
    "#Changed didgital output file\n",
    "h1_reader = harp.create_reader('harp_resources/h1-device.yml', epoch=harp.REFERENCE_EPOCH)\n",
    "h2_reader = harp.create_reader('harp_resources/h2-device.yml', epoch=harp.REFERENCE_EPOCH)\n",
    "session_data_reader = utils.SessionData(\"SessionSettings\")\n",
    "experiment_events_reader = utils.TimestampedCsvReader(\"ExperimentEvents\", columns=[\"Event\"])\n",
    "framecount_reader = utils.TimestampedCsvReader(\"OnixAnalogFrameCount\", columns=[\"Index\"])\n",
    "photometry_reader = utils.PhotometryReader(\"Processed_fluorescence\")\n",
    "video_reader = utils.Video(\"VideoData1\")\n",
    "onix_digital_reader = utils.TimestampedCsvReader(\"OnixDigital\", columns=[\"Clock\", \"HubClock\", \n",
    "                                                                         \"DigitalInputs0\",\n",
    "                                                                         \"DigitalInputs1\",\n",
    "                                                                         \"DigitalInputs2\",\n",
    "                                                                         \"DigitalInputs3\",\n",
    "                                                                         \"DigitalInputs4\",\n",
    "                                                                         \"DigitalInputs5\"\n",
    "                                                                         \"DigitalInputs6\",\n",
    "                                                                         \"DigitalInputs7\",\n",
    "                                                                         \"DigitalInputs8\",\n",
    "                                                                         \"Buttons\"])\n",
    "onix_harp_reader = utils.TimestampedCsvReader(\"OnixHarp\", columns=[\"Clock\", \"HubClock\", \"HarpTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "session_settings = utils.load_2(session_data_reader, data_path)\n",
    "\n",
    "print(session_settings.iloc[0]['metadata'].blocks[0].haltProtocol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
