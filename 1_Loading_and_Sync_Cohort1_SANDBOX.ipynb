{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Extract and align data from Onix, Harp, Sleap, and photometry\n",
    "## Cohort 1 and 2 working, Cohort 0: onix_digital Clock column is 0, explore why and/or use timestamps instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import harp\n",
    "import plotly.express as px\n",
    "from scipy.stats import mode\n",
    "\n",
    "import importlib\n",
    "import harp_resources.process\n",
    "import harp_resources.utils\n",
    "# Reassign to maintain direct references\n",
    "from harp_resources import process, utils\n",
    "\n",
    "from sleap import load_and_process as lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort0 = False #only read harp data when it exists, not in Cohort0 \n",
    "cohort2 = False\n",
    "\n",
    "#Cohort 1 vestibular mismatch, multiple OnixDigital files \n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/VestibularMismatch_day1/B6J2718-2024-12-12T13-28-14') #multiple onix_digital file\n",
    "\n",
    "#Cohort 1 vestibular mismatch, with clock accumulation issue marked on google sheet, seems fine though\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/VestibularMismatch_day1/B6J2719-2024-12-12T13-59-38') #multiple onix_digital file\n",
    "\n",
    "#Cohort 1 vestibular mismatch\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/VestibularMismatch_day1/B6J2717-2024-12-12T13-00-21')\n",
    "\n",
    "#Cohort 1 visual mismatch \n",
    "data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/Visual_mismatch_day3/B6J2718-2024-12-10T12-57-02') \n",
    "\n",
    "#Cohort 1 visual mismatch\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort1/Visual_mismatch_day3/B6J2717-2024-12-10T12-17-03')\n",
    "\n",
    "\n",
    "#Cohort 0 (no OnixHarp in this Cohort)\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort0/Cohort0_GCaMP_example/B3M3xx-2024-08-08T10-05-26')\n",
    "#cohort0 = True\n",
    "\n",
    "\n",
    "#Cohort 2 N.B. no videodata in this test set \n",
    "#cohort2 = True\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort2_like_test_data/2025-01-13T15-47-26')\n",
    "\n",
    "#Cohort 2 longer test YES OnixHarp! \n",
    "#N.B. no photometry in this test set (neitjer videos, but yes video_data)\n",
    "#cohort2 = True\n",
    "#data_path = Path('/Users/rancze/Documents/Data/vestVR/Cohort2_test_longer/2025-02-10T08-18-59')\n",
    " \n",
    "photometry_path = data_path.parent / f\"{data_path.name}_processedData\" / \"photometry\"\n",
    "\n",
    "#h1_datafolder = data_path / 'HarpDataH1' #only if reading separate registers\n",
    "#h2_datafolder = data_path / 'HarpDataH2' #only if reading separate registers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#h1 and h2 only needed if timestamps are readed separately and not as all harp_streams\n",
    "#h1_reader = harp.create_reader('harp_resources/h1-device.yml', epoch=harp.REFERENCE_EPOCH)\n",
    "#h2_reader = harp.create_reader('harp_resources/h2-device.yml', epoch=harp.REFERENCE_EPOCH)\n",
    "\n",
    "session_settings_reader = utils.SessionData(\"SessionSettings\")\n",
    "experiment_events_reader = utils.TimestampedCsvReader(\"ExperimentEvents\", columns=[\"Event\"])\n",
    "onix_framecount_reader = utils.TimestampedCsvReader(\"OnixAnalogFrameCount\", columns=[\"Index\"])\n",
    "#photometry_reader = utils.PhotometryReader(\"Processed_fluorescence\")\n",
    "video_reader1 = utils.Video(\"VideoData1\")\n",
    "video_reader2 = utils.Video(\"VideoData2\")\n",
    "onix_digital_reader = utils.OnixDigitalReader(\"OnixDigital\", columns=[\"Value.Clock\", \"Value.HubClock\", \n",
    "                                                                         \"Value.DigitalInputs\",\n",
    "                                                                         \"Seconds\"])\n",
    "onix_harp_reader = utils.TimestampedCsvReader(\"OnixHarp\", columns=[\"Clock\", \"HubClock\", \"HarpTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print (\"Loading session settings\")\n",
    "session_settings = utils.load_2(session_settings_reader, data_path) #Andrew's, creates ugly df, but used in further analysis code\n",
    "print (\"Loading experiment events\")\n",
    "experiment_events = utils.load_2(experiment_events_reader, data_path)\n",
    "\n",
    "print (\"Loading processed fluorescence\")\n",
    "photometry_data=pd.read_csv(str(photometry_path)+'/Processed_fluorescence.csv')\n",
    "print (\"Loading processed fluorescence info\")\n",
    "photometry_info=pd.read_csv(str(photometry_path)+'/Info.csv')\n",
    "print (\"Loading processed fluorescence events\")\n",
    "photometry_events=pd.read_csv(str(photometry_path)+'/Events.csv')\n",
    "\n",
    "if not cohort2:\n",
    "    print (\"Loading video data 1\")\n",
    "    video_data1 = utils.load_2(video_reader1, data_path)\n",
    "    print (\"Loading video data 2\")\n",
    "    video_data2 = utils.load_2(video_reader2, data_path)\n",
    "\n",
    "# read Onix data \n",
    "print (\"Loading OnixDigital\")\n",
    "onix_digital = utils.load_2(onix_digital_reader, data_path)\n",
    "print (\"Loading OnixAnalogFrameClock\")\n",
    "onix_analog_framecount = utils.load_2(onix_framecount_reader, data_path)\n",
    "print (\"Loading OnixAnalogClock\")\n",
    "onix_analog_clock = utils.read_OnixAnalogClock(data_path)\n",
    "print (\"Loading OnixAnalogData\")\n",
    "onix_analog_data = utils.read_OnixAnalogData(data_path, channels = [0], binarise=True, method='adaptive', refractory = 300, flip=True, verbose=False) #method adaptive or threshold (which is hard threshold at 120), refractory to avoid multiple detections\n",
    "\n",
    "#read HARP data\n",
    "print (\"Loading H1 and H2 streams\")\n",
    "harp_streams = utils.load_registers(data_path, dataframe = True) #loads as df, or if False, as dict \n",
    "\n",
    "#read syncronising signal between HARP and ONIX\n",
    "if not cohort0:\n",
    "    print (\"Loading OnixHarp\")\n",
    "    onix_harp = utils.load_2(onix_harp_reader, data_path)\n",
    "    onix_harp = utils.detect_and_remove_outliers(\n",
    "    df=onix_harp,\n",
    "    x_column=\"HarpTime\",\n",
    "    y_column=\"Clock\",\n",
    "    verbose=False  # True prints all outliers\n",
    "    )\n",
    "    onix_harp[\"HarpTime\"] = onix_harp[\"HarpTime\"] + 1 # known issue with current version of ONIX, harp timestamps lag 1 second\n",
    "    print (\"Warning: HarpTime +1s to account for know issue with ONIX\")\n",
    "\n",
    "print (\"Done Loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(harp_resources.utils)\n",
    "importlib.reload(harp_resources.process)\n",
    "\n",
    "# Reassign to maintain direct references\n",
    "from harp_resources import process, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(\n",
    "    conversions, \n",
    "    photometry_sync_events, \n",
    "    harp_to_onix_clock, \n",
    "    onix_time_to_photometry, \n",
    "    onix_to_harp_timestamp,\n",
    "    photometry_to_harp_time\n",
    ") = process.photometry_harp_onix_synchronisation(\n",
    "    onix_analog_data=onix_analog_data,\n",
    "    onix_analog_clock=onix_analog_clock,\n",
    "    onix_analog_framecount=onix_analog_framecount,\n",
    "    onix_digital=onix_digital,\n",
    "    onix_harp=onix_harp,\n",
    "    photometry_events=photometry_events,\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def resample_to_1khz_grid(experiment_events, photometry_data, onix_analog_clock, onix_analog_data, harp_streams):\n",
    "    \"\"\"\n",
    "    Resamples all datasets to a uniform 1 kHz (1 ms resolution) time grid while preserving alignment.\n",
    "    - Numeric signals are resampled using interpolation.\n",
    "    - Boolean signals retain original timestamps.\n",
    "\n",
    "    Parameters:\n",
    "        experiment_events (DataFrame): Original event timestamps.\n",
    "        photometry_data (DataFrame): Photometry data (with a \"TimeStamp\" column in seconds).\n",
    "        onix_analog_clock (ndarray): ONIX timestamps (nanoseconds).\n",
    "        onix_analog_data (ndarray): ONIX data.\n",
    "        harp_streams (DataFrame): HARP-streamed data (both numeric and boolean).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary of aligned datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert photometry timestamps from seconds to datetime\n",
    "    photometry_data[\"Datetime\"] = pd.to_datetime(photometry_data[\"TimeStamp\"], unit=\"s\", origin=\"1900-01-01\")\n",
    "    photometry_data = photometry_data.set_index(\"Datetime\").drop(columns=[\"TimeStamp\"])\n",
    "\n",
    "    # Convert ONIX timestamps from nanoseconds to datetime\n",
    "    onix_time_index = pd.to_datetime(onix_analog_clock, unit=\"ns\", origin=\"1900-01-01\")\n",
    "\n",
    "    # Define a uniform 1 kHz time grid spanning the full experiment\n",
    "    min_time = min(\n",
    "        experiment_events.index.min(),\n",
    "        photometry_data.index.min(),\n",
    "        onix_time_index.min(),\n",
    "        harp_streams.index.min()\n",
    "    )\n",
    "    max_time = max(\n",
    "        experiment_events.index.max(),\n",
    "        photometry_data.index.max(),\n",
    "        onix_time_index.max(),\n",
    "        harp_streams.index.max()\n",
    "    )\n",
    "\n",
    "    # Create the 1 kHz common time grid\n",
    "    common_time_grid = pd.date_range(start=min_time, end=max_time, freq=\"1ms\")\n",
    "\n",
    "    # Debugging print to confirm grid size\n",
    "    print(f\"⚡ Resampling to 1 kHz grid: {len(common_time_grid)} time points\")\n",
    "\n",
    "    # Split harp_streams into numeric and boolean columns\n",
    "    harp_numeric = harp_streams.select_dtypes(exclude=['bool'])\n",
    "    harp_bool = harp_streams.select_dtypes(include=['bool'])\n",
    "\n",
    "    # Resample numeric data to the 1 kHz grid\n",
    "    def resample_numeric(df):\n",
    "        \"\"\"Interpolates numeric data to match the 1 kHz time grid.\"\"\"\n",
    "        return df.reindex(df.index.union(common_time_grid)).interpolate(method='time').reindex(common_time_grid)\n",
    "\n",
    "    # Resample event timestamps, photometry, and ONIX signals\n",
    "    experiment_events_resampled = resample_numeric(experiment_events)\n",
    "    photometry_data_resampled = resample_numeric(photometry_data)\n",
    "\n",
    "    # Interpolate ONIX analog data\n",
    "    onix_analog_data_resampled = pd.DataFrame(\n",
    "        index=common_time_grid,\n",
    "        data=np.interp(\n",
    "            common_time_grid.astype('int64') / 1e9,  # Convert ms timestamps to seconds\n",
    "            onix_analog_clock / 1e9,  # Convert ONIX timestamps to seconds\n",
    "            onix_analog_data\n",
    "        ),\n",
    "        columns=[\"onix_analog_data\"]\n",
    "    )\n",
    "\n",
    "    # Resample numeric columns of harp_streams to the 1 kHz grid\n",
    "    harp_numeric_resampled = resample_numeric(harp_numeric)\n",
    "\n",
    "    # Keep boolean columns at original timestamps (no downsampling)\n",
    "    harp_bool_aligned = harp_bool.reindex(harp_bool.index.union(common_time_grid)).fillna(method='ffill')\n",
    "\n",
    "    return {\n",
    "        \"experiment_events_resampled\": experiment_events_resampled,\n",
    "        \"photometry_data_resampled\": photometry_data_resampled,\n",
    "        \"onix_analog_data_resampled\": onix_analog_data_resampled,\n",
    "        \"harp_numeric_resampled\": harp_numeric_resampled,\n",
    "        \"harp_bool_aligned\": harp_bool_aligned\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example Usage:\n",
    "aligned_data = resample_to_1khz_grid(experiment_events, photometry_data, onix_analog_clock, onix_analog_data, harp_streams)\n",
    "\n",
    "# # Access the resampled datasets:\n",
    "# experiment_events_resampled = aligned_data[\"experiment_events_resampled\"]\n",
    "# photometry_data_resampled = aligned_data[\"photometry_data_resampled\"]\n",
    "# onix_analog_data_resampled = aligned_data[\"onix_analog_data_resampled\"]\n",
    "# harp_numeric_resampled = aligned_data[\"harp_numeric_resampled\"]\n",
    "# harp_bool_aligned = aligned_data[\"harp_bool_aligned\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_events = experiment_events[\"Event\"].unique()\n",
    "print(unique_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plotting Parameters ----\n",
    "window_start = -1  # seconds, analysis window to plot and average\n",
    "window_stop = 5\n",
    "how_many_to_plot = -1  # -1 plots all or X plots first x halt events \n",
    "\n",
    "\n",
    "if \"Visual_mismatch\" in str(data_path):    \n",
    "    block_start_event = \"DrumWithReverseHalt block started\"\n",
    "    halt_event = \"Apply halt: 2s\"\n",
    "    block_end_event = \"Block timer elapsed\" # Set to \"no_end\" to scan all events\n",
    "\n",
    "if \"VestibularMismatch\" in str(data_path):\n",
    "    block_start_event = \"Sync signal started\"\n",
    "    halt_event = \"DrumWithReverseflow block started\"\n",
    "    block_end_event = \"no_end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from scipy.stats import mode\n",
    "\n",
    "\n",
    "def pad_arrays(array_list):\n",
    "    \"\"\"Pads a list of 1D NumPy arrays to the same length using NaN padding.\"\"\"\n",
    "    max_len = max(map(len, array_list))  # Efficient max length calculation\n",
    "    padded_array = np.empty((len(array_list), max_len), dtype=np.float64)  # Preallocate array\n",
    "    padded_array.fill(np.nan)  # Fill with NaNs in one operation\n",
    "\n",
    "    for i, arr in enumerate(array_list):\n",
    "        padded_array[i, :len(arr)] = arr  # Vectorized assignment\n",
    "\n",
    "    return padded_array\n",
    "\n",
    "# ---- Extract Halt Events Efficiently ----\n",
    "block_starts = experiment_events.query(\"Event == @block_start_event\").index.to_numpy()\n",
    "\n",
    "halt_events_list = []\n",
    "for block_start in block_starts:\n",
    "    if block_end_event == \"no_end\":\n",
    "        block_halts = experiment_events.query(\"Event == @halt_event and index > @block_start\")\n",
    "    else:\n",
    "        block_end = experiment_events.query(\"Event == @block_end_event and index > @block_start\").index.min()\n",
    "        if pd.notna(block_end):\n",
    "            block_halts = experiment_events.query(\"Event == @halt_event and index > @block_start and index < @block_end\")\n",
    "        else:\n",
    "            block_halts = pd.DataFrame()  # No valid end event found\n",
    "    \n",
    "    if not block_halts.empty:\n",
    "        halt_events_list.append(block_halts)\n",
    "\n",
    "block_halts = pd.concat(halt_events_list) if halt_events_list else pd.DataFrame()\n",
    "\n",
    "if block_halts.empty:\n",
    "    raise ValueError(f\"⚠️ No [{halt_event}] events found between [{block_start_event}] and [{block_end_event}]. \"\n",
    "                     f\"Check if the event names are correct and exist in experiment_events.\")\n",
    "\n",
    "# Convert Halt Times to NumPy for Efficiency\n",
    "halt_event_times = block_halts.index.to_numpy()\n",
    "\n",
    "# Adjust how_many_to_plot if it exceeds available events\n",
    "if how_many_to_plot > len(block_halts):\n",
    "    print(f\"⚠️ Warning: Requested {how_many_to_plot} halts, but only {len(block_halts)} are available. \"\n",
    "          \"Adjusting how_many_to_plot accordingly.\")\n",
    "    how_many_to_plot = len(block_halts)\n",
    "\n",
    "if how_many_to_plot == -1:\n",
    "    how_many_to_plot = len(block_halts)  # Limit to avoid excessive plots\n",
    "print(f\"Found {len(block_halts)} halt events within valid blocks, plotting {how_many_to_plot}.\")\n",
    "\n",
    "# Define colors\n",
    "flow_x_color, flow_y_color, photodiode_color = \"blue\", \"orange\", \"grey\"  # Changed photodiode to grey\n",
    "z_470_color, z_560_color = \"green\", \"red\"\n",
    "\n",
    "# Initialize lists for aligned data\n",
    "aligned_time = np.linspace(window_start, window_stop, 500)  \n",
    "flow_x_aligned, flow_y_aligned, photodiode_aligned, z_470_aligned, z_560_aligned = [], [], [], [], []\n",
    "\n",
    "# ----------------------\n",
    "# First Plot: Individual Trials\n",
    "# ----------------------\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for idx, halt_time in enumerate(block_halts.index[:how_many_to_plot]):\n",
    "    halt_time_seconds = halt_time.timestamp()\n",
    "    min_time, max_time = halt_time + pd.DateOffset(seconds=window_start), halt_time + pd.DateOffset(seconds=window_stop)\n",
    "\n",
    "    # Extract Optical Tracking Data\n",
    "    optical_x = harp_streams['OpticalTrackingRead0X(46)'].loc[min_time:max_time].dropna()\n",
    "    optical_y = harp_streams['OpticalTrackingRead0Y(46)'].loc[min_time:max_time].dropna()\n",
    "\n",
    "    if not optical_x.empty and not optical_y.empty:\n",
    "        optical_x_rel = (optical_x.index.astype(\"int64\") / 1e9) - halt_time_seconds\n",
    "        optical_y_rel = (optical_y.index.astype(\"int64\") / 1e9) - halt_time_seconds\n",
    "\n",
    "        label_x, label_y = \"Flow X\" if idx == 0 else None, \"Flow Y\" if idx == 0 else None\n",
    "        ax1.plot(optical_x_rel, optical_x, color=flow_x_color, alpha=0.3, label=label_x)\n",
    "        ax1.plot(optical_y_rel, optical_y, color=flow_y_color, alpha=0.3, label=label_y)\n",
    "\n",
    "        # Restrict aligned_time to the valid range of optical_x_rel\n",
    "        valid_mask = (aligned_time >= optical_x_rel.min()) & (aligned_time <= optical_x_rel.max())\n",
    "        aligned_time_valid = aligned_time[valid_mask]\n",
    "\n",
    "        # Interpolate only within the valid time range\n",
    "        flow_x_interp = np.interp(aligned_time_valid, optical_x_rel, optical_x, left=np.nan, right=np.nan)\n",
    "        flow_y_interp = np.interp(aligned_time_valid, optical_y_rel, optical_y, left=np.nan, right=np.nan)\n",
    "\n",
    "        # Append only the valid interpolated values\n",
    "        flow_x_aligned.append(flow_x_interp)\n",
    "        flow_y_aligned.append(flow_y_interp)\n",
    "\n",
    "ax1.set_xlabel(\"Relative Time (s)\")\n",
    "ax1.set_ylabel(\"Tracking Readout\")\n",
    "ax1.set_title(\"Optical Flow, Photodiode, and Photometry\")\n",
    "\n",
    "# Create separate y-axis for Photodiode\n",
    "ax3 = ax1.twinx()\n",
    "ax3.spines.right.set_position((\"outward\", 60))\n",
    "ax3.set_ylabel(\"Photodiode Signal\")\n",
    "ax3.set_ylim([0, 1.2])\n",
    "\n",
    "for idx, halt_time in enumerate(block_halts.index[:how_many_to_plot]):\n",
    "    halt_time_seconds = halt_time.timestamp()\n",
    "\n",
    "    onix_sec_start_time = harp_to_onix_clock(block_halts.iloc[idx][\"Seconds\"] + window_start)\n",
    "    onix_sec_stop_time = harp_to_onix_clock(block_halts.iloc[idx][\"Seconds\"] + window_stop)\n",
    "\n",
    "    onix_sec_start_index = np.searchsorted(onix_analog_clock, onix_sec_start_time)\n",
    "    onix_sec_stop_index = np.searchsorted(onix_analog_clock, onix_sec_stop_time)\n",
    "\n",
    "    onix_time_rel = (onix_to_harp_timestamp(onix_analog_clock[onix_sec_start_index:onix_sec_stop_index])\n",
    "                     .astype(\"int64\") / 1e9) - halt_time_seconds\n",
    "\n",
    "    photodiode_signal = onix_analog_data[onix_sec_start_index:onix_sec_stop_index]\n",
    "\n",
    "    label_photodiode = \"Photodiode\" if idx == 0 else None\n",
    "    ax3.plot(onix_time_rel, photodiode_signal, color=photodiode_color, alpha=0.5, label=label_photodiode)\n",
    "\n",
    "    # Restrict aligned_time to valid range of onix_time_rel\n",
    "    valid_mask = (aligned_time >= onix_time_rel.min()) & (aligned_time <= onix_time_rel.max())\n",
    "    aligned_time_valid = aligned_time[valid_mask]\n",
    "\n",
    "    # Interpolate photodiode on the valid time range\n",
    "    photodiode_interp = np.interp(aligned_time_valid, onix_time_rel, photodiode_signal, left=np.nan, right=np.nan)\n",
    "    photodiode_aligned.append(photodiode_interp)\n",
    "    \n",
    "# Create second y-axis for photometry\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Fluorescence Signal\")\n",
    "\n",
    "if \"TimeStamp\" in photometry_data.columns:\n",
    "    photometry_data = photometry_data.set_index(\"TimeStamp\")\n",
    "\n",
    "for idx, halt_time in enumerate(block_halts.index[:how_many_to_plot]):\n",
    "    halt_time_seconds = halt_time.timestamp()\n",
    "\n",
    "    photometry_sec_start_time = onix_time_to_photometry(harp_to_onix_clock(block_halts.iloc[idx][\"Seconds\"] + window_start))\n",
    "    photometry_sec_stop_time = onix_time_to_photometry(harp_to_onix_clock(block_halts.iloc[idx][\"Seconds\"] + window_stop))\n",
    "\n",
    "    photometry_sec = photometry_data.loc[photometry_sec_start_time:photometry_sec_stop_time]\n",
    "\n",
    "    if not photometry_sec.empty:\n",
    "        photometry_time_rel = (photometry_to_harp_time(photometry_sec.index).astype(\"int64\") / 1e9) - halt_time_seconds\n",
    "\n",
    "        label_560, label_470 = \"z_560\" if idx == 0 else None, \"z_470\" if idx == 0 else None\n",
    "        ax2.plot(photometry_time_rel, photometry_sec['z_560'], color=z_560_color, alpha=0.3, label=label_560)\n",
    "        ax2.plot(photometry_time_rel, photometry_sec['z_470'], color=z_470_color, alpha=0.3, label=label_470)\n",
    "        \n",
    "        # Restrict aligned_time to the valid range of photometry_time_rel\n",
    "        valid_mask = (aligned_time >= photometry_time_rel.min()) & (aligned_time <= photometry_time_rel.max())\n",
    "        aligned_time_valid = aligned_time[valid_mask]\n",
    "\n",
    "        # Perform interpolation on the adjusted time range\n",
    "        z_560_interp = np.interp(aligned_time_valid, photometry_time_rel, photometry_sec['z_560'], left=np.nan, right=np.nan)\n",
    "        z_470_interp = np.interp(aligned_time_valid, photometry_time_rel, photometry_sec['z_470'], left=np.nan, right=np.nan)\n",
    "        \n",
    "        # Append only the valid interpolated values\n",
    "        z_560_aligned.append(z_560_interp)\n",
    "        z_470_aligned.append(z_470_interp)\n",
    "\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax3.legend(loc=\"center right\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# ----------------------\n",
    "# Second Plot: Averages with Error Shading (Proper Axes Labels & No Overlap)\n",
    "# ----------------------\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax1.set_xlabel(\"Relative Time (s)\")\n",
    "ax1.set_ylabel(\"Tracking Readout\")\n",
    "\n",
    "# Pad all signal data to ensure uniform shape\n",
    "flow_x_aligned_padded = pad_arrays(flow_x_aligned)\n",
    "flow_y_aligned_padded = pad_arrays(flow_y_aligned)\n",
    "photodiode_aligned_padded = pad_arrays(photodiode_aligned)\n",
    "z_560_aligned_padded = pad_arrays(z_560_aligned)\n",
    "z_470_aligned_padded = pad_arrays(z_470_aligned)\n",
    "\n",
    "# Ensure all arrays have the same length\n",
    "# Get the number of valid (non-NaN) values per time point across trials\n",
    "valid_counts_560 = np.sum(~np.isnan(z_560_aligned_padded), axis=0)\n",
    "valid_counts_470 = np.sum(~np.isnan(z_470_aligned_padded), axis=0)\n",
    "\n",
    "# Find the last point where at least 80% of trials still have data\n",
    "threshold = 0.8 * len(z_560_aligned_padded)  # Adjustable threshold (80%)\n",
    "adaptive_cutoff = np.where(valid_counts_560 >= threshold)[0][-1]  # Last valid index\n",
    "\n",
    "# Use the smaller of (1) standard min_length, (2) adaptive cutoff\n",
    "min_length = min(\n",
    "    aligned_time.shape[0], \n",
    "    photodiode_aligned_padded.shape[1], \n",
    "    z_560_aligned_padded.shape[1], \n",
    "    z_470_aligned_padded.shape[1], \n",
    "    flow_x_aligned_padded.shape[1],  \n",
    "    flow_y_aligned_padded.shape[1],\n",
    "    adaptive_cutoff  # Ensure we include this index\n",
    ")\n",
    "\n",
    "print(f\"🔍 Adaptive cutoff applied at index {adaptive_cutoff}, using min_length = {min_length}\")\n",
    "\n",
    "# Compute means after padding\n",
    "flow_x_mean = np.nanmean(flow_x_aligned_padded, axis=0)\n",
    "flow_y_mean = np.nanmean(flow_y_aligned_padded, axis=0)\n",
    "photodiode_mean = np.nanmean(photodiode_aligned_padded, axis=0)\n",
    "z_560_mean = np.nanmean(z_560_aligned_padded, axis=0)\n",
    "z_470_mean = np.nanmean(z_470_aligned_padded, axis=0)\n",
    "\n",
    "# Compute SEM\n",
    "flow_x_sem = np.nanstd(flow_x_aligned_padded, axis=0) / np.sqrt(np.sum(~np.isnan(flow_x_aligned_padded), axis=0))\n",
    "flow_y_sem = np.nanstd(flow_y_aligned_padded, axis=0) / np.sqrt(np.sum(~np.isnan(flow_y_aligned_padded), axis=0))\n",
    "photodiode_sem = np.nanstd(photodiode_aligned_padded, axis=0) / np.sqrt(np.sum(~np.isnan(photodiode_aligned_padded), axis=0))\n",
    "z_560_sem = np.nanstd(z_560_aligned_padded, axis=0) / np.sqrt(np.sum(~np.isnan(z_560_aligned_padded), axis=0))\n",
    "z_470_sem = np.nanstd(z_470_aligned_padded, axis=0) / np.sqrt(np.sum(~np.isnan(z_470_aligned_padded), axis=0))\n",
    "\n",
    "## Truncate all arrays to match min_length\n",
    "aligned_time = aligned_time[:min_length]\n",
    "flow_x_mean = flow_x_mean[:min_length]\n",
    "flow_y_mean = flow_y_mean[:min_length]\n",
    "photodiode_mean = photodiode_mean[:min_length]\n",
    "z_560_mean = z_560_mean[:min_length]\n",
    "z_470_mean = z_470_mean[:min_length]\n",
    "\n",
    "# Truncate SEM values to match min_length\n",
    "flow_x_sem = flow_x_sem[:min_length]\n",
    "flow_y_sem = flow_y_sem[:min_length]\n",
    "photodiode_sem = photodiode_sem[:min_length]\n",
    "z_560_sem = z_560_sem[:min_length]\n",
    "z_470_sem = z_470_sem[:min_length]\n",
    "\n",
    "# Plot flow means with SEM shading\n",
    "ax1.plot(aligned_time, flow_x_mean, color=flow_x_color, label=\"Flow X (Mean)\")\n",
    "ax1.fill_between(aligned_time, flow_x_mean - flow_x_sem, flow_x_mean + flow_x_sem, color=flow_x_color, alpha=0.2)\n",
    "\n",
    "ax1.plot(aligned_time, flow_y_mean, color=flow_y_color, label=\"Flow Y (Mean)\")\n",
    "ax1.fill_between(aligned_time, flow_y_mean - flow_y_sem, flow_y_mean + flow_y_sem, color=flow_y_color, alpha=0.2)\n",
    "\n",
    "# Photodiode Signal\n",
    "ax3 = ax1.twinx()\n",
    "ax3.set_ylabel(\"Photodiode Signal\")\n",
    "ax3.spines.right.set_position((\"outward\", 60))\n",
    "ax3.plot(aligned_time, photodiode_mean, color=photodiode_color, label=\"Photodiode (Mean)\")\n",
    "ax3.fill_between(aligned_time, photodiode_mean - photodiode_sem, photodiode_mean + photodiode_sem, color=photodiode_color, alpha=0.2)\n",
    "\n",
    "# Photometry Fluorescence\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel(\"Fluorescence Signal\")\n",
    "ax2.plot(aligned_time, z_560_mean, color=z_560_color, label=\"z_560 (Mean)\")\n",
    "ax2.fill_between(aligned_time, z_560_mean - z_560_sem, z_560_mean + z_560_sem, color=z_560_color, alpha=0.2)\n",
    "\n",
    "ax2.plot(aligned_time, z_470_mean, color=z_470_color, label=\"z_470 (Mean)\")\n",
    "ax2.fill_between(aligned_time, z_470_mean - z_470_sem, z_470_mean + z_470_sem, color=z_470_color, alpha=0.2)\n",
    "\n",
    "ax1.legend(loc=\"upper left\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "ax3.legend(loc=\"center right\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "onix_harp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aeon",
   "language": "python",
   "name": "aeon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
